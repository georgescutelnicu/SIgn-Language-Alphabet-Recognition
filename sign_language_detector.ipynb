{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6RKAYJZBN1M",
        "outputId": "b615546b-87b8-4142-baed-4f4e7af02aa5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting unrar\n",
            "  Downloading unrar-0.4-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: unrar\n",
            "Successfully installed unrar-0.4\n"
          ]
        }
      ],
      "source": [
        "!pip install unrar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kgQ6e-VVBQ2f"
      },
      "outputs": [],
      "source": [
        "!unrar x '/content/drive/MyDrive/Sign Language/Dataset/Alphabet.rar'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCuH9SN9g_b8"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b398asfFibx-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad2982fb-308e-4ad0-ddb9-5d6a90ff4acd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting mediapipe\n",
            "  Downloading mediapipe-0.9.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.0/33.0 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.8/dist-packages (from mediapipe) (22.2.0)\n",
            "Collecting flatbuffers>=2.0\n",
            "  Downloading flatbuffers-23.1.21-py2.py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from mediapipe) (3.2.2)\n",
            "Requirement already satisfied: protobuf<4,>=3.11 in /usr/local/lib/python3.8/dist-packages (from mediapipe) (3.19.6)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.8/dist-packages (from mediapipe) (4.6.0.66)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from mediapipe) (1.21.6)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mediapipe) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mediapipe) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mediapipe) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mediapipe) (0.11.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib->mediapipe) (1.15.0)\n",
            "Installing collected packages: flatbuffers, mediapipe\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 1.12\n",
            "    Uninstalling flatbuffers-1.12:\n",
            "      Successfully uninstalled flatbuffers-1.12\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.9.2 requires flatbuffers<2,>=1.12, but you have flatbuffers 23.1.21 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed flatbuffers-23.1.21 mediapipe-0.9.1.0\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "  import mediapipe as mp\n",
        "except:\n",
        "  !pip install mediapipe\n",
        "  import mediapipe as mp"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mediapipe setup**"
      ],
      "metadata": {
        "id": "4-ifKcl3QXli"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q82coLCgisZb"
      },
      "outputs": [],
      "source": [
        "DATASET = '/content/Alphabet'\n",
        "\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "mp_drawing_styles = mp.solutions.drawing_styles\n",
        "mp_hands = mp.solutions.hands\n",
        "\n",
        "hands = mp_hands.Hands(\n",
        "        static_image_mode=True,\n",
        "        max_num_hands=2,\n",
        "        min_detection_confidence=0.3)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data preparation**"
      ],
      "metadata": {
        "id": "dmsM9Ls9QaEt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_WWXyYvpz6J"
      },
      "outputs": [],
      "source": [
        "# Create data/labels arrays\n",
        "data = []\n",
        "labels = []\n",
        "\n",
        "# Loop through each directory(each letter)\n",
        "for dir_ in os.listdir(DATASET):\n",
        "  # Loop through each image from the directory\n",
        "    for img_path in os.listdir(os.path.join(DATASET, dir_)):\n",
        "      # Create arrays for data\n",
        "        data = []\n",
        "        x_ = []\n",
        "        y_ = []\n",
        "\n",
        "        # Read the image\n",
        "        img = cv2.imread(os.path.join(DATASET, dir_, img_path))\n",
        "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Get the hand landmarks\n",
        "        results = hands.process(img_rgb)\n",
        "\n",
        "        # Check if there was a landmark\n",
        "        if results.multi_hand_landmarks:\n",
        "          # Loop through each landmark\n",
        "            for hand_landmarks in results.multi_hand_landmarks:\n",
        "              # Loop through the coord of the landmarks and save them to x_ and y_\n",
        "                for i in range(len(hand_landmarks.landmark)):\n",
        "                    x = hand_landmarks.landmark[i].x\n",
        "                    y = hand_landmarks.landmark[i].y\n",
        "\n",
        "                    x_.append(x)\n",
        "                    y_.append(y)\n",
        "              # Loop through the coord of the landmarks and perform normalization\n",
        "              # We literally substract the min of x/y of the landmark from each x/y of it\n",
        "              # In order for our model to learn faster and better\n",
        "                for i in range(len(hand_landmarks.landmark)):\n",
        "                    x = hand_landmarks.landmark[i].x\n",
        "                    y = hand_landmarks.landmark[i].y\n",
        "                    data.append(x - min(x_))\n",
        "                    data.append(y - min(y_))\n",
        "\n",
        "          # We append the data and the label to the original arrays\n",
        "            data.append(data)\n",
        "            labels.append(dir_)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Save/Load the hand landmarks for each image**"
      ],
      "metadata": {
        "id": "6wSpywaEQiae"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7dk1m3_NlE3g"
      },
      "outputs": [],
      "source": [
        "with open('data.pickle', 'wb') as f:\n",
        "  pickle.dump({'data': data, 'labels': labels}, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78zYT77R23hC"
      },
      "outputs": [],
      "source": [
        "data_dict = pickle.load(open('/content/data.pickle', 'rb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Turn the data into np arrays**"
      ],
      "metadata": {
        "id": "sMIBt6TOQw-_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBC528MwJorM"
      },
      "outputs": [],
      "source": [
        "data = np.asarray(data_dict['data'])\n",
        "labels = np.asarray(data_dict['labels'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Split the data**"
      ],
      "metadata": {
        "id": "MdRXh0ksQ04j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xrE6hZCSZo6u"
      },
      "outputs": [],
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, shuffle=True, stratify=labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_tngMLjFkbi"
      },
      "source": [
        "**For a few images there were more than one hand it seems so the size of the array will be bigger than the others and it wont work. We delete the arrays with more than one hand detected (21 landmarks) (x + y = 42)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "inXb9Nkcv_nF"
      },
      "outputs": [],
      "source": [
        "train_idx = []\n",
        "for e, j in enumerate(x_train):\n",
        "  if len(j) != 42:\n",
        "    train_idx.append(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtGzi4UP5-Lj"
      },
      "outputs": [],
      "source": [
        "test_idx = []\n",
        "for e, j in enumerate(x_test):\n",
        "  if len(j) != 42:\n",
        "    test_idx.append(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1oEf4mRR9twU"
      },
      "outputs": [],
      "source": [
        "x_train = np.delete(x_train, train_idx)\n",
        "y_train = np.delete(y_train, train_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e03HRzD4-u1_"
      },
      "outputs": [],
      "source": [
        "x_test = np.delete(x_test, test_idx)\n",
        "y_test = np.delete(y_test, test_idx)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create model and predict on the test set**"
      ],
      "metadata": {
        "id": "NhCsJlI5Q4BR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IL83xuQIpfNK",
        "outputId": "8be5272e-9e72-4b69-890f-161ffb28a835"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier()"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "model = RandomForestClassifier()\n",
        "model.fit(x_train.tolist(), y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VSrIdg3vVQX",
        "outputId": "904382fa-c80b-40a0-9a15-d66cadebdd34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "97.82193958664547% of samples were classified correctly !\n"
          ]
        }
      ],
      "source": [
        "y_predict = model.predict(x_test.tolist())\n",
        "\n",
        "score = accuracy_score(y_predict, y_test)\n",
        "\n",
        "print(f'{score * 100} of samples were classified correctly !')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Save/Load the model**"
      ],
      "metadata": {
        "id": "rUuwsMxCSR_C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-RL7Rak5EoFk"
      },
      "outputs": [],
      "source": [
        "f = open('model.p', 'wb')\n",
        "pickle.dump({'model': model}, f)\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IozBElOUJ4eH"
      },
      "outputs": [],
      "source": [
        "model_dict = pickle.load(open('/content/model.p', 'rb'))\n",
        "model = model_dict['model']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Predict on a video**"
      ],
      "metadata": {
        "id": "uJn7u6QBSYGm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNpnHlIoCHIB"
      },
      "outputs": [],
      "source": [
        "# Get the path of the input/output videos\n",
        "in_video = '/content/in.mp4'\n",
        "out_video = '/content/out.avi'\n",
        "\n",
        "# Mediapipe and videocapture setup\n",
        "cap = cv2.VideoCapture(in_video)\n",
        "\n",
        "mp_hands = mp.solutions.hands\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "mp_drawing_styles = mp.solutions.drawing_styles\n",
        "\n",
        "hands = mp_hands.Hands(static_image_mode=True, min_detection_confidence=0.4)\n",
        "\n",
        "ret, frame = cap.read()\n",
        "\n",
        "H, W, _ = frame.shape\n",
        "\n",
        "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "\n",
        "out = cv2.VideoWriter(out_video, fourcc,  30.0, (W, H))\n",
        "\n",
        "while True:\n",
        "    # Create data/labels arrays\n",
        "    data = []\n",
        "    x_ = []\n",
        "    y_ = []\n",
        "\n",
        "    # Get the frame of the video, if there are no frames left exit the loop\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "     break\n",
        "    \n",
        "    # Frame shape\n",
        "    H, W, _ = frame.shape\n",
        "    # Convert the frame from BGR to RGB\n",
        "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Get the hand landmarks\n",
        "    results = hands.process(frame_rgb)\n",
        "\n",
        "    # Check if there was a landmark\n",
        "    if results.multi_hand_landmarks:\n",
        "\n",
        "        # Loop through each landmark and draw them on the image\n",
        "        for hand_landmarks in results.multi_hand_landmarks:\n",
        "            mp_drawing.draw_landmarks(\n",
        "                frame,  # image to draw\n",
        "                hand_landmarks,  # model output\n",
        "                mp_hands.HAND_CONNECTIONS,  # hand connections\n",
        "                mp_drawing_styles.get_default_hand_landmarks_style(),\n",
        "                mp_drawing_styles.get_default_hand_connections_style())\n",
        "            \n",
        "        # Loop through each landmark\n",
        "        for hand_landmarks in results.multi_hand_landmarks:\n",
        "            # Loop through the coord of the landmarks and save them to x_ and y_\n",
        "            for i in range(len(hand_landmarks.landmark)):\n",
        "                x = hand_landmarks.landmark[i].x\n",
        "                y = hand_landmarks.landmark[i].y\n",
        "\n",
        "                x_.append(x)\n",
        "                y_.append(y)\n",
        "\n",
        "            # Loop through the coord of the landmarks and perform normalization\n",
        "            # We literally substract the min of x/y of the landmark from each x/y of it\n",
        "            # In order for our model to learn faster and better\n",
        "            for i in range(len(hand_landmarks.landmark)):\n",
        "                x = hand_landmarks.landmark[i].x\n",
        "                y = hand_landmarks.landmark[i].y\n",
        "                data.append(x - min(x_))\n",
        "                data.append(y - min(y_))\n",
        "\n",
        "        # Get the coord of the min and max landmark point in order to draw a rectangle box\n",
        "        x1 = int(min(x_) * W) - 10\n",
        "        y1 = int(min(y_) * H) - 10\n",
        "\n",
        "        x2 = int(max(x_) * W) + 10\n",
        "        y2 = int(max(y_) * H) + 10\n",
        "\n",
        "        # Make the prediction\n",
        "        prediction = str(model.predict([np.asarray(data)]))\n",
        "\n",
        "        # Frame the hand into a rectangle box and predict the label\n",
        "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 0), 4)\n",
        "        cv2.putText(frame, prediction, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 1.3, (0, 0, 0), 3,\n",
        "                    cv2.LINE_AA)\n",
        "\n",
        "    # Write the frame to the video\n",
        "    out.write(frame)\n",
        "    cv2.waitKey(1)\n",
        "\n",
        "# Close the writing of the video file\n",
        "cap.release()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}